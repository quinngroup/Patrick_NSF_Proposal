
%%%%%%%%% PROPOSAL -- 15 pages (including Results from Prior NSF Support)

\required{Project Description}

% From the NSF Grants Proposal Guide:
% "The Project Description should provide a clear statement of the work 
% to be undertaken and must include the objectives for the period of the 
% proposed work and expected significance; the relationship of this work
% to the present state of knowledge in the field, as well as to work in 
% progress by the PI under other support.
%
% The Project Description should outline the general plan of work, 
% including the broad design of activities to be undertaken, and, 
% where appropriate, provide a clear description of experimental 
% methods and procedures. Proposers should address 

% what they want to do,
% why they want to do it,
% how they plan to do it,
% how they will know if they succeed,
% and what benefits could accrue if the project is successful.

% The project activities may be based
% on previously established and/or innovative methods and approaches,
% but in either case must be well justified. These issues apply to 
% both the technical aspects of the proposal and the way in which
% the project may make broader contributions."

\section{Mining Pre-Exposure Prophylaxis Trends in Social Media}

Pre-Exposure Prophylaxis (PrEP) is a recently developed method for the prevention of Human Immunodeficiency Virus (HIV) via the administration of an oral pharmaceutical trade named Truvada. Truvada contains active ingredients tenofovir and emtricitabine, both nucleotide reverse transcriptase inhibitors (NRTIs). In the last four years, since Truvada was approved for PrEP in 2012, PrEP has shown demonstrated efficacy at preventing HIV for HIV negative individuals in serodiscordant relationships\cite{liu2014early}.

Initial studies of PrEP have shown that it is highly effective\cite{golub2013efficacy}, however because it is still a new treatment, it is facing a number of medical and social obstacles before it reaches full adoption. Incomplete clinical and patient understanding, social stigma, and uncertain insurance status have been identified as challenges preventing continued adoption\cite{calabrese2015stigma}. Also, since Truvada is an oral NRTI, it must be taken daily. Cases where patients do not adhere to their full prescription have led to loss of viral protection, and some patients and clinicians worry that lack of adherence could lead to increased risk of infection with drug resistant strains\cite{arnold2012qualitative}.

In order to identify factors that lead to the direct mitigation of HIV infection, we would like to be able to access direct medical data, however doing so can be difficult due to privacy regulations. Additionally, medical data often does not contain direct unfiltered opinions and feedback which capture patient and public perception. For this reason we propose the use of social media as one of our principle sources of data. Though we have investigated the use of multiple social media platforms including Facebook, Grindr, Reddit and Twitter, we have focused primarily on the use of Twitter for past projects and intend to use Twitter for future analyses. Twitter is especially useful because in addition to textual data, various useful metadata is also available including datetime, geolocation, username, hastags, and external hyperlinks. We have used many of these metadata attributes in previous analyses and will continue to do so in future analyses. Twitter also features a convenient well documented and free API and has over 300 million monthly active users.

In past analyses we collected over 1 million tweets from the Twitter streaming API filtered on PrEP and HIV related keywords. By using embedding techniques such as Word2Vec and Doc2Vec, we were able to identify new keywords and hashtags that are relevant to the PrEP conversation on Twitter including unexpected political connections "NancyReagan" and popular hashtags associated with PrEP that might not have been known to researchers in advance such as "\#whereisprep" (see table 1 for examples of hashtags and structurally-related tweets). In addition Doc2Vec can be used to query the top N tweets related to a given hashtag or the top N users related to a given hashtag. This allowed us to identify a small subset of the N tweets most structurally related to "PrEP" that can be human-read without requiring humans to read the full dataset. Reading from the set of structurally most important tweets, uncovered important blog articles describing some of the fears of drug resistant forms of HIV and concerns as to whether these strains could be caused in part by over use or misuse of PrEP medications. The linked blog articles that we found highlight the usefulness of the hyperlink metadata embedded in tweets. Through these hyperlinks, Twitter acts as an index, providing indirect access to a much larger external social media ecosystem.

% Doc2Vec table
\begin{table}
\centering
\caption{Cosine similarity to document-vector ``\#PrEP''}
\begin{tabular}{|l|c|} \hline
Related hashtag/tweet & Cosine similarity to \#PrEP\\ \hline
\#lgbtmedia16 & 0.739128\\ \hline
\#hiv & 	0.727602 \\ \hline
\#whereisprep & 0.707165 \\ \hline
\#truvada & 0.696113 \\ \hline
\#hivprevention & 0.636068 \\ \hline
tweet-702179860983189504 & 0.630055\\ \hline
user-711275699529764864 & 0.629254\\ \hline
tweet-708519265540907010 & 0.628778 \\ \hline
tweet-712032637024653313 & 0.628646 \\ \hline
\#harrogatehour & 0.628547 \\ \hline
\hline\end{tabular}
\label{tbl:d2v}
\end{table}

Our Doc2Vec results also allowed us to identify the top N users most relevant to PrEP. By querying the Twitter REST API for these users' timelines, and using topic modeling methods like Latent Dirichlet Analysis (LDA), we identified the word-distribution-topics present in the Twitter conversation (see figure 1). This analysis went beyond the simple keyword identification analysis from Word2Vec since it clustered keywords into topics and it operated on all tweets that PrEP-related users tweeted, not just PrEP related tweets. The LDA results showed a variety of related concerns such as other sexually transmitted diseases, LGBT related topics, health insurance and political topics. Neither PrEP or Truvada were present in the top 30 keywords related to HIV/AIDS demonstrating that PrEP is still a nascent rare topic in the online discussion. An extension to LDA, Dynamic Topic Modeling (DTM), was able to capture topic and word frequency over time. The DTM results showed that the keyword "PrEP" is increasing in relative frequency over time, even relative to related words such as "pill", "prevention" and "drug". This demonstrates increased interest in PrEP which may correlate with an increase in PrEP adoption over the data acquisition period.

\begin{figure*}
\centering
\includegraphics[height=3in, width=5.0in]{figures/fixFig5}
\caption{LDA topic modeling for the top 500 users related to PrEP.}
\label{fig:lda}
\end{figure*}

Using an open dataset of tweets which were labeled with binary sentiment labels, we performed a sentiment analysis. This analysis allowed us to identify N PrEP related tweets with the highest sentiment, and N PrEP tweets with the lowest sentiment. After performing the automated sentiment analysis, a human was added into the loop to quickly read the top positive and negative tweets to get a sense of the successes and issues present in public perception. In the positive tweets we found hyperlinks to blogs with positive firsthand accounts from individuals successfully using PrEP to stay HIV negative. In the negative tweets we found concerns of whether Truvada can protect against drug resistant strains of HIV (example negative tweets shown in table 2). Together these approaches and results show that we can take raw text and metadata and extract keywords, hastags, temporal trends, and sentiment information. Doc2Vec and sentiment classification allow the researcher to extract a set of the N most highly relevant tweets from a large corpus that can be easily human-readable.

% do a top negative tweets table (also with tweet ID)
% table 4
\begin{table*}
\centering
\caption{Negative sentiment tweets.}
\begin{tabular}{|p{2.5cm}|p{10cm}|} \hline
Category & Text\\ \hline
General & ``Also, how f***ing vile of Hillary to say. Reagan did f***ing NOTHING during the AIDS epidemic until it was too late. What a stupid old hag.''\\ \hline
General & ``I wonder why he beat her a** when she was tryna leave like she wasn't gone be running back when she found out she had HIV \& nobody want her''\\ \hline
General & ``Aaannd. Hillary Clinton breathes a sigh of relief that Twitter has left its outrage of her AIDS comments behind to tend to Drumpf debacle.''\\ \hline

PrEP specific & ``RT gaston\_croupier \#Truvada patent's not expired yet but it is sold online as a generic drug? There's something rotten in internet \#PrEP h''\\ \hline
PrEP specific & ``Equality\_MI Syph \& Hep C have gone up 550\% in Gay Men bc many feel tht bc they're on PrEP, they don't need condoms. HIV isn't the only STI.''\\ \hline
PrEP specific & ``Xaviom8 in interviews he says he was adherent. strain was highly resistant, and Truvada wouldn't have blocked it anyways. PrEP didn't fail.''\\ \hline

Truvada specific & ``not surprised at all that someone got HIV on truvada. people get pregnant on birth control. tomato-condoms are still important-tomahto''\\ \hline
Truvada specific & ``Now reading that truvada does not protect against certain strains of the HIV virus. Yet people want to take that risk..''\\ \hline
Truvada specific & ``I think I have conjunctivitis unless truvada cured it overnight cuz im not feeling as horrible today as last night''\\ \hline

\hline\end{tabular}
\label{tbl:neg}
\end{table*}


Though our previous research on data mining social media has uncovered important tweets, keywords, sentiments and hashtags related to the Twitter discussion of PrEP, many important questions remain poorly understood. One important issue is determining why patients stop adhering to their PrEP medication. While our LDA results uncovered "stigma" and other related keywords, and some of the critical tweets we identified described uncertainty in the efficacy of PrEP, this question still remains to be fully answered. One of the challenges with using Twitter as a data source is that we cant verify ground truth labels on the people authoring the tweets. For example we don't know if they are taking Truvada, or whether they are HIV negative or positive, or other important details of their medical status. By incorporating direct medical data we can identify connections between sentiment and written opinions with more concrete medical outcomes.

Other extensions to our previous analysis could include the investigation of additional diseases or behaviors that could correlate with risk of developing HIV. One of the overall goals of biosurveilance is to predict disease outbreaks before they happen in order to intervene preventatively. An outbreak in Scott county Indiana in early 2016 was thought to have been caused by drug usage. In a town of 4,000 people 135 people were diagnosed with HIV, and about 80\% of those diagnosed were codiagnosed with hepatitis C. In theory some online social activity may have been able to indirectly identify this outbreak before it happened\cite{conrad2015community}. In order to identify and predict these hot-spots we can make use of the geolocation metadata, and also do network analyses to identify subgroups, and how information and via inference, social interactions propagate through these subgroups. Some first steps along these lines have already been made by our collaborators \cite{young2014methods}.

The benefit of our results over these other results is that we have started to take advantage of qualitative information in the natural language of the tweet without relying on labels or simple filtering strategies. If we take advantage of some of the natural language processing techniques described in our previous work, combined with geolocation data and hard medical outcomes, we could build a more accurate predictor of HIV outbreaks. We could also use these predictive models to identify the attributes contributing to increased risk and use public health efforts to alleviate those issues. Ultimately we hope to reduce the spread of HIV overall and also predict and prevent acute outbreaks before and as they are happening.

Finally, our previous research on mining qualitative sentiments surrounding disease treatment provides an important contribution to the larger data science community. We have shown a specific application where we mine social sentiment to identify what is working and what the challenges are for PrEP, but a simmilar framework could easily be taken and applied to improve the social barriers surrounding some other disease treatment like chemotherapy. Parmacetical companies and hospitals can use our open source code with minimal modification to monitor their disease and treatment of interest to monitor and improve the outcomes and happiness of their patients.


\section{Semi supervised learning and operations on arbitrary dimension tensors (Oak Ridge, Arvind Ramanathan}

In January 2017 I will start working at Oak Ridge National Laboratory on a data science related fellowship. The goals of the grant supporting me seeks to use novel CPU/GPU hybrid chips to develope novel computational models in the area of semi-supervised deep learning. While I don't understand all of the details of the research proposal, or the specifics of the hardware at the time of writing (early November 2015), I will attempt to give a broad explanation of the research area and some possible directions.

Semi-supervised learning refers to a situation where both labeled and unlabeled data are used to train a discriminative model\cite{zhu2011semi}. In contrast supervised learning uses only labeled data to train a discriminative model and unsupervised learning uses only unlabeled data to train model that captures some structure of the data distribution ie clustering or generative. Supervised learning is often on of the most used and useful types of machine learning since the model is directly predicting a label that has external research value. However, producing that labeled data can often require human annotation or physical experimentation with can be costly. Because labeled data is costly, machine learning models in many domains are error-limited by a lack labeled data to train on\cite{guillaumin2010multimodal}, which is perhaps ironic in a world of so called "big data". By taking advantage of both an unsupervised model of the data-distribution using cheap unlabeled data, and a discriminative model of the labels given the data on some small labeled dataset, semi supervised learning can produce results better than a n approach that uses only supervised learning.

Let me give an example of a semi supervised learning method in action, and for added consistency, I'll use an example from my own work described in the previous section (Mining Pre-Exposure Prophylaxis Trends in Social Media). When we did the sentiment analysis in the previous section we actually did 2 discrete steps. First we used doc2vec to transform each tweet into a dense high dimensional doc-vector, then we used a very simple logistic regression model to classify the tweet doc vectors into positive or negative sentiments. As a human reading the tweets in the results, you might be surprised at the relative accuracy of the classification, especially considering that logistic regression is relatively speaking one of the simplest supervised classifiers, and the natural language present in the tweets is very terse and complicated for such a simple supervised logistic regression model to work so well. Doc2vec, an unsupervised embedding method produced dense document vectors that were able to capture the distribution of the tweet data, mapping tweets that had co-occuring word semantics to be "close" in a high dimensional space. Thus the combination of unsupervised embedding and supervised classification yielded better results than a simple supervised classification could have done on its own (we didn't try doing classification without using doc2vec in the PrEP paper, but see the original doc2vec paper for relative quantification of accuracy gained by embedding\cite{le2014distributed}). Embedding, which is just one example of semi-supervised learning has become very widely used as a single step in a larger deep learning model (see the last section in this proposal below where we use embedding in a bioinformatics deep learning context) also others\cite{weston2012deep}.

Though we know that we will be using novel hardware at Oak Ridge National Laboratory we aren't currently sure of the hardware details. We know that we will have access to new CPU / GPU hybrid chips that are being produced by Nvidia. GPUs have become increasingly popular in machine learning in the last couple of years, especially in the area of deep learning. This is largely because general purpose programming APIs such as CUDA have allowed researchers to take advantage of cost efficient computing power present in GPU's for computation-bound tasks (such as deep learning). On Nvidia's Blog an article titled "Accelerating AI with GPUs: A New Computing Model" describes how typical deep learning models can be trained 50x faster on a Tesla M40 GPU than on a typical CPU. Though Nvidia typically provides a compiler toolchain supporting a language called CUDA with basic C-like syntax, Nvidia has also provided a series of highly optimized routines for things like linear algebra, matrix multiplication, convolution and manipulation that allow researchers to think in terms of linear algebra primitives on matrices instead of low level array-wise manipulation\cite{chetlur2014cudnn}. This makes construction of machine learning models in low-level CUDA code very analogous to using popular higher level machine learning libraries in languages such as Python.

In order to provide support for semi-supervised learning on these novel CPU/GPU hybrid chips, we want to develop computational operations and building blocks that allow for arbitrary dimension tensors. Though most machine learning models use operations on matrices or 3-tensors, some theoretical and practical applications could take advantage of operations on tensors of arbitrary dimension. Think of the embedding example above. We embedded a 1 dimensional sequence of word-tokens (a document) into a dense 1 dimensional real-valued vector representation. In other situations we may want to produce arbitrary dimensional embeddings as a building block to construct semi supervised deep learning models. Several papers show examples at a theoretical level, where operations, projections and decompositions on high rank tensors can be used to extract feature information from the original data\cite{cichocki2016low, cichocki2014tensor}. These operations such as Principle Components Analysis, Multi-Dimentional Scaling, Locally Preserving Projection, Neighborhood Preserving projection etc. could be used to provide embeddings to be used in a larger deep learning network either by preprocessing, or by composing the objective function of the embedding kernel with the other parts of the deep learning network in order to train the whole network through back propagation.

Practically we could in theory build small programs that implement these tensor operations in a small CUDA or C-like program. However, recent development of deep learning frameworks gives us another practical implementation option. Frameworks such as Caffe, Theano, Torch and Tensorflow allow machine learning researchers to build and share efficient machine learning models that take advantage of a simple scripting-level API, and take advantage of highly parallel efficient machine implementations. Of these frameworks, the one that I am most familiar with is Tensorflow, an open source project supported by Google \cite{abadi2016tensorflow}. 

Tensorflow is currently under fast paced development, but already it offers support for data-flow parallelism, distributed and multi-device computing, automatic differentiation, and it offers C++ and Python based APIs for specifying models. Creating a deep learning model in Tensorflow is done by composing building blocks, or "layers", into a graph. In the graph operations are nodes and tensors are edges that feed out of one node and into another node. In a typical supervised learning situation data flows in a directed acyclic path from the input node(s) to the objective or classification node(s). Through automatic differentiation, the error or "loss" in the objective function is used to produce parameter updates through gradient decent. These updates are then propogated backwards through the graph in the direction opposite data-flow. This iterative process repeats many times as the models parameters are trained.

For high throughput and flexibility the compute graph's operations can be assigned to various "devices" such as CPU or GPU. These devices need not be co-located on the same machine, and during the evaluation of the graph tensors that are output from one operation that are needed on another machine or device will be transported over a bus or over a network connection. The same operation can be instantiated on different devices by instantiating a "kernel" for the given device. The kernel is usually specified in C, C++ or CUDA-C for performance and for compatibility with the device. Once these operations and kernels are instantiated, they can be made available along with the other kernels present in Tensorflow to provide yet another building block for other researchers to use to build their own compute-graphs. Thus once we implement our high rank tensor operation(s), we can relatively easily use our operation to extend Tensorflow which makes our operations available for many other researchers in the community.

Ultimatly in addition to building embedding and arbitrary tensor operations to extend computational capabilities for other machine learning researchers, we also want to produce some specific applications to help solve our surveillance problems. Since we don't know the exact details of the hardware at this point, it is unclear exactly what our capabilities will be. However I think that a basic re-implementation of existing methods like word2vec or other deep learning components, perhaps in a way that uses high-rank tensors would provide a familiar embedding-like method that could be used on social media data such as described in the first section of this proposal. By using our newly developed algorithm to do embedding or sentiment classification on twitter data, we can perhaps uncover new trends and mechanisms underlying the spread of disease. This would help facilitate the growing need for data analysis mechanisms in the biological and medical sciences\cite{cichocki2014era}. Ultimately we will use these new methods to predict disease outbreaks, prevent disease, and identify social issues and concerns surrounding existing diseases and treatments with the goal of improving the social outcomes in addition to medical outcomes for patients.


\section{Using machine on instagram images, comments and image-tags to monitor mental health}

- Look at Keith's research and written exam section and think of creative ways to use Machine Learning on this dataset....
- Mention Instagram public api

\section{Generating novel peptides based on unsupervised latent distribution of peptides}

- Link partially written paper on this.
- There will be overlap between oral slideshow and student seminar presentation

\required{Broader Impacts}
% As in the project summary, broader impacts of the proposed work
% must be called out separately in the project description.  
% You may be able to give more specific examples, 
% or discuss how you've previously achieved these impacts.
% It should be similar, but not identical, to the Broader Impacts statement
% in the project summary.